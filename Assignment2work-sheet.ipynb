{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [default]",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    },
    "colab": {
      "name": "Assignment 2 work-sheet.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ihXumA44nAqg"
      },
      "source": [
        "## Functions\n",
        "- For questions 5,6,7 use the function `levenshtein`\n",
        "- For question 6, modify the function `levenshtein` on the variable `substitutions`\n",
        "- For question 8, use the function `jaro_winkler`. The function is defined in the file `Edistance.py`\n",
        "- For questions 5 to 10, the function `uniFreq` is needed to calculate the count of unigrams in the corpus $C_3$\n",
        "- For question 9, the function `bigramFreq` is needed to calculate the count of bigrams in the corpus $C_3$\n",
        "- For question 10, use the code snippet given in the last cell\n",
        "\n",
        "## Files\n",
        "- use unigram.csv for questions 5,6,7,8\n",
        "- use bigrams.csv for questions 9,10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4uHVyRGnAqj"
      },
      "source": [
        "def uniFreq():\n",
        "    unigrams = open('unigram.csv').read().splitlines()\n",
        "    wordFreq = dict()\n",
        "    for word in unigrams:\n",
        "        item = word.split(',')\n",
        "        wordFreq[item[0].strip()] = int(item[1].strip())\n",
        "    return wordFreq\n",
        "\n",
        "def bigramFreq():\n",
        "    bigrams =  open('bigrams.csv').read().splitlines()\n",
        "    wordBigram = dict()\n",
        "    for word in bigrams:\n",
        "        item = word.split(',')\n",
        "        wordBigram[item[0].strip()] = int(item[1].strip())\n",
        "    return wordBigram"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EttBUrj3nAql"
      },
      "source": [
        "## levenshtein"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "xvuuGf5_nAqm"
      },
      "source": [
        "# Function Definition starts with def <function name> (<input arguments>)\n",
        "# 2 strings to comapre, hence 2 inputs as arguments\n",
        "\n",
        "# mind the indentation\n",
        "def levenshtein(s1, s2):\n",
        "    if len(s1) < len(s2):\n",
        "        return levenshtein(s2, s1)\n",
        "\n",
        "    # len(s1) >= len(s2)\n",
        "    if len(s2) == 0:\n",
        "        return len(s1)\n",
        "\n",
        "    previous_row = range(len(s2) + 1)\n",
        "\n",
        "    for i, c1 in enumerate(s1):\n",
        "        current_row = [i + 1]\n",
        "        for j, c2 in enumerate(s2):\n",
        "            # cost for each of 3 operations.\n",
        "            insertions = previous_row[j + 1] + 1 # j+1 instead of j since previous_row and current_row are one character longer\n",
        "            deletions = current_row[j] + 1       # than s2\n",
        "            substitutions = previous_row[j] + (c1 != c2) # if true returns one, oterwise 0\n",
        "            \n",
        "            current_row.append(min(insertions, deletions, substitutions))\n",
        "        previous_row = current_row\n",
        "    \n",
        "    return previous_row[-1]"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDJjNXVqrR1f"
      },
      "source": [
        "from collections import defaultdict\n",
        "_range = range\n",
        "def jaro_winkler(ying, yang, long_tolerance=False, winklerize=True):\n",
        "    ying_len = len(ying)\n",
        "    yang_len = len(yang)\n",
        "\n",
        "    if not ying_len or not yang_len:\n",
        "        return 0\n",
        "\n",
        "    min_len = max(ying_len, yang_len)\n",
        "    search_range = (min_len // 2) - 1\n",
        "    if search_range < 0:\n",
        "        search_range = 0\n",
        "\n",
        "    ying_flags = [False]*ying_len\n",
        "    yang_flags = [False]*yang_len\n",
        "\n",
        "    # looking only within search range, count & flag matched pairs\n",
        "    common_chars = 0\n",
        "    for i, ying_ch in enumerate(ying):\n",
        "        low = i - search_range if i > search_range else 0\n",
        "        hi = i + search_range if i + search_range < yang_len else yang_len - 1\n",
        "        for j in _range(low, hi+1):\n",
        "            if not yang_flags[j] and yang[j] == ying_ch:\n",
        "                ying_flags[i] = yang_flags[j] = True\n",
        "                common_chars += 1\n",
        "                break\n",
        "\n",
        "    # short circuit if no characters match\n",
        "    if not common_chars:\n",
        "        return 0\n",
        "\n",
        "    # count transpositions\n",
        "    k = trans_count = 0\n",
        "    for i, ying_f in enumerate(ying_flags):\n",
        "        if ying_f:\n",
        "            for j in _range(k, yang_len):\n",
        "                if yang_flags[j]:\n",
        "                    k = j + 1\n",
        "                    break\n",
        "            if ying[i] != yang[j]:\n",
        "                trans_count += 1\n",
        "    trans_count /= 2\n",
        "\n",
        "    # adjust for similarities in nonmatched characters\n",
        "    common_chars = float(common_chars)\n",
        "    weight = ((common_chars/ying_len + common_chars/yang_len +\n",
        "              (common_chars-trans_count) / common_chars)) / 3\n",
        "\n",
        "    # winkler modification: continue to boost if strings are similar\n",
        "    if winklerize and weight > 0.7 and ying_len > 3 and yang_len > 3:\n",
        "        # adjust for up to first 4 chars in common\n",
        "        j = min(min_len, 4)\n",
        "        i = 0\n",
        "        while i < j and ying[i] == yang[i] and ying[i]:\n",
        "            i += 1\n",
        "        if i:\n",
        "            weight += i * 0.1 * (1.0 - weight)\n",
        "\n",
        "        # optionally adjust for long strings\n",
        "        # after agreeing beginning chars, at least two or more must agree and\n",
        "        # agreed characters must be > half of remaining characters\n",
        "        if (long_tolerance and min_len > 4 and common_chars > i+1 and\n",
        "                2 * common_chars >= min_len + i):\n",
        "            weight += ((1.0 - weight) * (float(common_chars-i-1) / float(ying_len+yang_len-i*2+2)))\n",
        "\n",
        "    return weight"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cw2erVNinAqo"
      },
      "source": [
        "## Jaro Winkler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb0AEiQJnAqp",
        "outputId": "d3c1fcea-a48c-4d30-bf70-ddf30173f57b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#!python 'https://github.com/shallabhkhera/NPTEL-NLP/blob/shallabhkhera-assignment-2/Edistance.py?raw=true'\n",
        "#from Edistance import jaro_winkler\n",
        "print jaro_winkler('bimal','vimal')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.866666666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXN5n1lbnAqq"
      },
      "source": [
        "### Jaro Winkler Distance\n",
        "\n",
        "The Jaro–Winkler distance is a measure of similarity between two strings.  The Jaro-Winkler similarity is given by `1 - Jaro Winkler distance`. The Jaro–Winkler distance metric is designed and best suited for short strings such as person names. The similarity score is normalized such that 0 equates to no similarity and 1 is an exact match.\n",
        "\n",
        "$ d_{j}=\\left\\{{\\begin{array}{ll}0&{\\text{if }}m=0\\\\{\\frac  {1}{3}}\\left({\\frac  {m}{|s_{1}|}}+{\\frac  {m}{|s_{2}|}}+{\\frac  {m-t}{m}}\\right)&{\\text{otherwise}}\\end{array}}\\right. $\n",
        "\n",
        "\n",
        "Where \n",
        "- **s1** and **s2** are the strings \n",
        "- **m** is the number of matching characters (see below);\n",
        "- **t** is half the number of transpositions (see below).\n",
        "\n",
        "Source - [Jaro-Winkler Distance (Wikipedia)](https://en.wikipedia.org/wiki/Jaro%E2%80%93Winkler_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1rMBQKfnAqt"
      },
      "source": [
        "## Bigram Likelihood"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtKt8mPanAqu",
        "outputId": "5a332933-c1b6-453a-c842-ebde6f82bb1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "source": [
        "wordFreq = uniFreq()\n",
        "wordBigram = bigramFreq()\n",
        "def bigram_likelihood(bigramWord1,word1,word2,uniDict=wordFreq,biDict=wordBigram):\n",
        "    bi = biDict[bigramWord1]\n",
        "    uni = uniDict[word1]\n",
        "\n",
        "    print bi,uni\n",
        "\n",
        "    bigramProb = bi/(uni*1.0)\n",
        "    return bigramProb\n",
        "\n",
        "bigram_likelihood('iron safe','iron','safe')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IOError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-5a41e08b269c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwordFreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniFreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mwordBigram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbigramFreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mbigram_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbigramWord1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mword2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0muniDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordFreq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbiDict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwordBigram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mbi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbigramWord1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0muni\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muniDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-a7a8409fc77c>\u001b[0m in \u001b[0;36muniFreq\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0muniFreq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0munigrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unigram.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mwordFreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munigrams\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIOError\u001b[0m: [Errno 2] No such file or directory: 'unigram.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I65RG6ymnAqv"
      },
      "source": [
        "## Add-one smoothing for finding likelihood of a sentence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz_JY3AbnAqw",
        "outputId": "c51af4a9-e119-4095-8303-2991835a0508"
      },
      "source": [
        "wordFreq['<s>'] = 1\n",
        "wordFreq['</s>'] = 1\n",
        "stri2 = ['<s> sandip babu sang bande mataram </s>','<s> chandranath babu asked for betel leaves </s>','<s> poor bimala went to the dressing room </s>']\n",
        "for stri in stri2:\n",
        "    mult = 1.0\n",
        "    for i,item in enumerate(stri.split(' ')):\n",
        "        try:\n",
        "            print (wordBigram[item+' '+stri.split()[i+1]] + 1)/((wordFreq[item] + len(wordFreq.keys()))*1.0),item+' '+stri.split()[i+1]\n",
        "            mult = mult * (wordBigram[item+' '+stri.split()[i+1]] + 1)/((wordFreq[item] + len(wordFreq.keys()))*1.0)\n",
        "        except:\n",
        "            try:\n",
        "                print (1)/((wordFreq[item] + len(wordFreq.keys()))*1.0),item+' '+stri.split()[i+1]\n",
        "                mult = mult * (1)/((wordFreq[item] + len(wordFreq.keys()))*1.0)\n",
        "            except:\n",
        "                print item\n",
        "    print mult"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.00980244307043 <s> sandip\n",
            "0.00905109489051 sandip babu\n",
            "0.000149075730471 babu sang\n",
            "0.000150784077201 sang bande\n",
            "0.00584883023395 bande mataram\n",
            "0.000149970005999 mataram </s>\n",
            "0.000150806816468 </s>\n",
            "1.74932821106e-18\n",
            "0.000904840898809 <s> chandranath\n",
            "0.00180695678362 chandranath babu\n",
            "0.000149075730471 babu asked\n",
            "0.0001495215311 asked for\n",
            "for\n",
            "0.000301477238468 betel leaves\n",
            "0.000150625094141 leaves </s>\n",
            "0.000150806816468 </s>\n",
            "1.65494105455e-21\n",
            "0.00120645453174 <s> poor\n",
            "0.00045051809581 poor bimala\n",
            "0.000297707651087 bimala went\n",
            "0.00014905351021 went to\n",
            "0.000150693188668 to the\n",
            "0.000150715900528 the dressing\n",
            "0.00105342362679 dressing room\n",
            "0.000148345942738 room </s>\n",
            "0.000150806816468 </s>\n",
            "8.56025744882e-29\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}